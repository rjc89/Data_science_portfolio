{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import some modules from scipy for the calculation of skew, normality and pearsons R value\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.1})\n",
    "sns.set_context(\"paper\", font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/constar/Documents/GitHub/Data_Science_Portfolio/House_Price_Regression_Kaggle/train.csv')\n",
    "test = pd.read_csv('/Users/constar/Documents/GitHub/Data_Science_Portfolio/House_Price_Regression_Kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../input/train.csv')\n",
    "# test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "y_train = train.SalePrice.values # store the target in a separate variable\n",
    "y_mean = np.mean(y_train)         # calculate the mean as a base model for the target\n",
    "id_test = test.Id                #pull out the test Id for the Id column of the submission\n",
    "\n",
    "#log transform the target:\n",
    "y_train = np.log1p(y_train)\n",
    "\n",
    "num_train = len(train)  # store the no of training examples\n",
    "df_all = pd.concat([train, test])  #concat the training and testing data to one dataframe\n",
    "df_all.drop(['Id', 'SalePrice'], axis=1, inplace=True) # drop the Id and SalePrice columns from the training data\n",
    "\n",
    "df_all = pd.get_dummies(df_all, drop_first=True) # get dummies for non-numeric features\n",
    "\n",
    "# deal with skewed features\n",
    "\n",
    "\n",
    "#log transform skewed numeric features:\n",
    "numeric_feats = df_all.dtypes[df_all.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = df_all[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "df_all[skewed_feats] = np.log1p(df_all[skewed_feats])\n",
    "\n",
    "# df_all = pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "df_all = df_all.fillna(df_all.mean())\n",
    "\n",
    "\n",
    "\n",
    "train = df_all[:num_train]\n",
    "test = df_all[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #log transform the target:\n",
    "# train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# #log transform skewed numeric features:\n",
    "# numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "# skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "# skewed_feats = skewed_feats.index\n",
    "\n",
    "# all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "# In [7]:\n",
    "# all_data = pd.get_dummies(all_data)\n",
    "# In [8]:\n",
    "# #filling NA's with the mean of the column:\n",
    "# all_data = all_data.fillna(all_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1460 entries, 0 to 1459\n",
      "Columns: 245 entries, 1stFlrSF to Utilities_NoSeWa\n",
      "dtypes: float64(199), int64(10), uint8(36)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "class StackingCVRegressorAveraged(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors, meta_regressor, n_folds=5):\n",
    "        self.regressors = regressors\n",
    "        self.meta_regressor = meta_regressor\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.regr_ = [list() for x in self.regressors]\n",
    "        self.meta_regr_ = clone(self.meta_regressor)\n",
    "\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.regressors)))\n",
    "\n",
    "        for i, clf in enumerate(self.regressors):\n",
    "            for train_idx, holdout_idx in kfold.split(X, y):\n",
    "                instance = clone(clf)\n",
    "                self.regr_[i].append(instance)\n",
    "\n",
    "                instance.fit(X[train_idx], y[train_idx])\n",
    "                y_pred = instance.predict(X[holdout_idx])\n",
    "                out_of_fold_predictions[holdout_idx, i] = y_pred\n",
    "\n",
    "        self.meta_regr_.fit(out_of_fold_predictions, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([r.predict(X) for r in regrs]).mean(axis=1)\n",
    "            for regrs in self.regr_\n",
    "        ])\n",
    "        return self.meta_regr_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class for out of fold training of meta-model\n",
    "\n",
    "class StackingCVRegressorRetrained(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors, meta_regressor, n_folds=5, use_features_in_secondary=False):\n",
    "        self.regressors = regressors\n",
    "        self.meta_regressor = meta_regressor\n",
    "        self.n_folds = n_folds\n",
    "        self.use_features_in_secondary = use_features_in_secondary\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.regr_ = [clone(x) for x in self.regressors]\n",
    "        self.meta_regr_ = clone(self.meta_regressor)\n",
    "\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.regressors)))\n",
    "\n",
    "        # Create out-of-fold predictions for training meta-model\n",
    "        for i, regr in enumerate(self.regr_):\n",
    "            for train_idx, holdout_idx in kfold.split(X, y):\n",
    "                instance = clone(regr)\n",
    "                instance.fit(X[train_idx], y[train_idx])\n",
    "                out_of_fold_predictions[holdout_idx, i] = instance.predict(X[holdout_idx])\n",
    "\n",
    "        # Train meta-model\n",
    "        if self.use_features_in_secondary:\n",
    "            self.meta_regr_.fit(np.hstack((X, out_of_fold_predictions)), y)\n",
    "        else:\n",
    "            self.meta_regr_.fit(out_of_fold_predictions, y)\n",
    "        \n",
    "        # Retrain base models on all data\n",
    "        for regr in self.regr_:\n",
    "            regr.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            regr.predict(X) for regr in self.regr_\n",
    "        ])\n",
    "\n",
    "        if self.use_features_in_secondary:\n",
    "            return self.meta_regr_.predict(np.hstack((X, meta_features)))\n",
    "        else:\n",
    "            return self.meta_regr_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for averaging the regressors \n",
    "\n",
    "class AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, regressors):\n",
    "        self.regressors = regressors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.regr_ = [clone(x) for x in self.regressors]\n",
    "        \n",
    "        # Train base models\n",
    "        for regr in self.regr_:\n",
    "            regr.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            regr.predict(X) for regr in self.regr_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup stacked regressors\n",
    "\n",
    "en = make_pipeline(RobustScaler(), SelectFromModel(Lasso(alpha=0.03)), ElasticNet(alpha=0.001, l1_ratio=0.1))\n",
    "    \n",
    "rf = RandomForestRegressor(n_estimators=250, n_jobs=4, min_samples_split=25, min_samples_leaf=25, max_depth=3)\n",
    "                           \n",
    "et = ExtraTreesRegressor(n_estimators=100, n_jobs=4, min_samples_split=25, min_samples_leaf=35, max_features=150)\n",
    "\n",
    "xgbm = xgb.sklearn.XGBRegressor(max_depth=4, learning_rate=0.005, subsample=0.9, base_score=y_mean,\n",
    "                                objective='reg:linear', n_estimators=1000)\n",
    "                           \n",
    "stack_avg = StackingCVRegressorAveraged((en, rf, et), ElasticNet(l1_ratio=0.1, alpha=1.4))\n",
    "\n",
    "stack_with_feats = StackingCVRegressorRetrained((en, rf, et), xgbm, use_features_in_secondary=True)\n",
    "\n",
    "stack_retrain = StackingCVRegressorRetrained((en, rf, et), ElasticNet(l1_ratio=0.1, alpha=1.4))\n",
    "\n",
    "averaged = AveragingRegressor((en, rf, et, xgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 0.8623 (0.0264)\n",
      "RandomForest score: 0.7563 (0.0101)\n",
      "ExtraTrees score: 0.7940 (0.0249)\n",
      "XGBoost score: -9257820.2293 (741622.8402)\n",
      "Averaged base models score: -578616.3328 (46361.4931)\n",
      "Stacking (with primary feats) score: -9257972.5035 (741740.1546)\n",
      "Stacking (retrained) score: -0.0033 (0.0052)\n",
      "Stacking (averaged) score: -0.0031 (0.0051)\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "\n",
    "results = cross_val_score(en, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"ElasticNet score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(rf, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"RandomForest score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(et, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"ExtraTrees score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(xgbm, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"XGBoost score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(averaged, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"Averaged base models score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(stack_with_feats, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"Stacking (with primary feats) score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "\n",
    "results = cross_val_score(stack_retrain, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"Stacking (retrained) score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
    "                 \n",
    "results = cross_val_score(stack_avg, train.values, y_train, cv=5, scoring='r2')\n",
    "print(\"Stacking (averaged) score: %.4f (%.4f)\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
